{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "data = scipy.io.loadmat(\"data/spam_data.mat\")\n",
    "names = [x[0] for x in data['names'][:,1:].ravel()]\n",
    "X_train = data['P_train'].transpose().astype(\"float\")\n",
    "X_test = data['P_test'].transpose().astype(\"float\")\n",
    "y_train = (data['T_train'].transpose().ravel() + 1)/2\n",
    "y_test = (data['T_test'].transpose().ravel() + 1)/2\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3681 observations with 57 dimensions\n",
      "Train with approximately 81 epochs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'pack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e2b55041c7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_centr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mexp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output_layer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'pack'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load the MNIST data\n",
    "#X_train and y_train refers to the usual 60.000 by 784 matrix and 60.000 vector\n",
    "#X_test and y_test refers to the usual 10.000 by 784 and 10.000 vector\n",
    "# X_test = np.loadtxt('/home/rob/Dropbox/ml_projects/RCNN/MNIST_data/X_test.csv', delimiter=',')\n",
    "# y_test = np.loadtxt('/home/rob/Dropbox/ml_projects/RCNN/MNIST_data/y_test.csv', delimiter=',')\n",
    "# X_train = np.loadtxt('/home/rob/Dropbox/ml_projects/RCNN/MNIST_data/X_train.csv', delimiter=',')\n",
    "# y_train = np.loadtxt('/home/rob/Dropbox/ml_projects/RCNN/MNIST_data/y_train.csv', delimiter=',')\n",
    "\n",
    "\"\"\"Hyper-parameters\"\"\"\n",
    "batch_size = 300            # Batch size for stochastic gradient descent\n",
    "test_size = batch_size      # Temporary heuristic. In future we'd like to decouple testing from batching\n",
    "num_centr = 150             # Number of \"hidden neurons\" that is number of centroids\n",
    "max_iterations = 1000       # Max number of iterations\n",
    "learning_rate = 5e-2        # Learning rate\n",
    "num_classes = 10            # Number of target classes, 10 for MNIST\n",
    "var_rbf = 225               # What variance do you expect workable for the RBF?\n",
    "\n",
    "#Obtain and proclaim sizes\n",
    "N,D = X_train.shape         \n",
    "Ntest = X_test.shape[0]\n",
    "print('We have %s observations with %s dimensions'%(N,D))\n",
    "\n",
    "#Proclaim the epochs\n",
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train with approximately %d epochs' %(epochs))\n",
    "\n",
    "#Placeholders for data\n",
    "x = tf.placeholder('float',shape=[batch_size,D],name='input_data')\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size], name = 'Ground_truth')\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Hidden_layer\") as scope:\n",
    "  #Centroids and var are the main trainable parameters of the first layer\n",
    "  centroids = tf.Variable(tf.random_uniform([num_centr,D],dtype=tf.float32),name='centroids')\n",
    "  var = tf.Variable(tf.truncated_normal([num_centr],mean=var_rbf,stddev=5,dtype=tf.float32),name='RBF_variance')\n",
    "  \n",
    "  #For now, we collect the distanc\n",
    "  exp_list = []\n",
    "  for i in range(num_centr):\n",
    "        exp_list.append(tf.exp((-1*tf.reduce_sum(tf.square(tf.subtract(x,centroids[i,:])),1))/(2*var[i])))\n",
    "        phi = tf.transpose(tf.pack(exp_list))\n",
    "        \n",
    "with tf.name_scope(\"Output_layer\") as scope:\n",
    "    w = tf.Variable(tf.truncated_normal([num_centr,num_classes], stddev=0.1, dtype=tf.float32),name='weight')\n",
    "    bias = tf.Variable( tf.constant(0.1, shape=[num_classes]),name='bias')\n",
    "        \n",
    "    h = tf.matmul(phi,w)+bias\n",
    "    size2 = tf.shape(h)\n",
    "\n",
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(h,y_)\n",
    "  cost = tf.reduce_sum(loss)\n",
    "  loss_summ = tf.scalar_summary(\"cross entropy_loss\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    tvars = tf.trainable_variables()\n",
    "    #We clip the gradients to prevent explosion\n",
    "    grads = tf.gradients(cost, tvars)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = zip(grads, tvars)\n",
    "    train_step = optimizer.apply_gradients(gradients)\n",
    "#     The following block plots for every trainable variable\n",
    "#      - Histogram of the entries of the Tensor\n",
    "#      - Histogram of the gradient over the Tensor\n",
    "#      - Histogram of the grradient-norm over the Tensor\n",
    "    numel = tf.constant([[0]])\n",
    "    for gradient, variable in gradients:\n",
    "      if isinstance(gradient, ops.IndexedSlices):\n",
    "        grad_values = gradient.values\n",
    "      else:\n",
    "        grad_values = gradient\n",
    "      \n",
    "      numel +=tf.reduce_sum(tf.size(variable))  \n",
    "        \n",
    "      h1 = tf.histogram_summary(variable.name, variable)\n",
    "      h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n",
    "      h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "with tf.name_scope(\"Evaluating\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(h,1), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "   \n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "# For now, we collect performances in a Numpy array.\n",
    "# In future releases, I hope TensorBoard allows for more\n",
    "# flexibility in plotting\n",
    "perf_collect = np.zeros((4,int(np.floor(max_iterations /100))))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    print('Start session')\n",
    "    writer = tf.train.SummaryWriter(\"/home/rob/Dropbox/ml_projects/RBFN_tf/log_tb\", sess.graph_def)\n",
    "\n",
    "    step = 0\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "#    #Debugging\n",
    "#    batch_ind = np.random.choice(N,batch_size,replace=False)\n",
    "#    result = sess.run([phi],feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "#    print(result[0])\n",
    "    \n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "      batch_ind = np.random.choice(N,batch_size,replace=False)\n",
    "      if i%100 == 1:\n",
    "        #Measure train performance\n",
    "        result = sess.run([cost,accuracy,train_step],feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "        perf_collect[0,step] = result[0]\n",
    "        perf_collect[2,step] = result[1]\n",
    "        \n",
    "        \n",
    "        #Measure test performance\n",
    "        test_ind = np.random.choice(Ntest,test_size,replace=False)\n",
    "        result = sess.run([cost,accuracy,merged], feed_dict={ x: X_test[test_ind], y_: y_test[test_ind]})\n",
    "        perf_collect[1,step] = result[0]\n",
    "        perf_collect[3,step] = result[1]\n",
    "      \n",
    "        #Write information for Tensorboard\n",
    "        summary_str = result[2]\n",
    "        writer.add_summary(summary_str, i)\n",
    "        writer.flush()  #Don't forget this command! It makes sure Python writes the summaries to the log-file\n",
    "        \n",
    "        #Print intermediate numbers to terminal\n",
    "        acc = result[1]\n",
    "        print(\"Estimated accuracy at iteration %s of %s: %s\" % (i,max_iterations, acc))\n",
    "        step += 1\n",
    "      else:\n",
    "        sess.run(train_step,feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "        \n",
    "\"\"\"Additional plots\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(perf_collect[2],label = 'Train accuracy')\n",
    "plt.plot(perf_collect[3],label = 'Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(perf_collect[0],label = 'Train cost')\n",
    "plt.plot(perf_collect[1],label = 'Test cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We can now open TensorBoard. Run the following line from your terminal\n",
    "# tensorboard --logdir=/home/rob/Dropbox/ml_projects/RBFN_tf/log_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
